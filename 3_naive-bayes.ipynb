{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "In deze opdracht ga je Naive Bayes gebruiken om het onderscheid tussen berichten uit twee verschillende nieuwsgroepen te leren.  \n",
    "Voor informatie over de dataset, zie: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# misc data processing imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "# dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# classifier & testing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# to parse regular expressions\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data laden\n",
    "\n",
    "  * Laad de train en test data van de 20newsgroups dataset. \n",
    "    * Vraag hierbij alleen de categorieën 'comp.sys.mac.hardware' en 'comp.sys.ibm.pc.hardware' op.\n",
    "    * Zorg er ook voor dat header, footers en quotes weggelaten worden.\n",
    "  * Hoeveel train samples (nieuwsgroep artikelen) zijn er? En hoeveel test samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "777"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = ['comp.sys.mac.hardware' ,'comp.sys.ibm.pc.hardware' ]\n",
    "datasetTrain = fetch_20newsgroups(subset='train', categories=categories,remove=('headers', 'footers', 'quotes','description'),shuffle=True, random_state=42)\n",
    "datasetTest = fetch_20newsgroups(subset='test', categories=categories,remove=('headers', 'footers', 'quotes','description'),shuffle=True, random_state=42)\n",
    "print(len(datasetTrain.data))\n",
    "print(len(datasetTest.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Count vectorizer\n",
    "\n",
    "Je gaat nu zelf de data omzetten naar count vectors, waarop Naive Bayes getraind kan worden.\n",
    "\n",
    "### 2.1 Pre-processing\n",
    "\n",
    "  * Splits elk newsgroup artikel in de train dataset in een lijst van afzonderlijke woorden.  \n",
    "  Vervang daarbij ook alle hoofdletters door kleine letters.\n",
    "  * Doe bovenstaande ook voor de test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsTrainlower= [ re.split('\\W+',lines.lower()) for lines in datasetTrain.data] \n",
    "dsTestlower= [ re.split('\\W+',lines.lower()) for lines in datasetTest.data] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Bouw een vocabulaire op van alle unieke woorden die in de complete **train** dataset voorkomen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabTrain=([word for sublist in dsTrainlower for word in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabTrain = list(set(vocabTrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Tel voor elk nieuwgroep artikel hoevaak elk woord uit de vocabulaire voorkomt.  \n",
    "  Het resultaat is een matrix met shape = [n\\_train\\_samples, n\\_words\\_in\\_vocabulary]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 2.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "WordFreqTR=np.zeros((len(datasetTrain.data),len(vocabTrain)))\n",
    "\n",
    "for idx1, trwords in enumerate(dsTrainlower):\n",
    "    for idx2, vword in enumerate(vocabTrain):\n",
    "        WordFreqTR[idx1,idx2]=trwords.count(vword)\n",
    "print(WordFreqTR)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Matrix = np.zeros((len(datasetTrain.data),len(vocabTrain)))\n",
    "Matrix.shape\n",
    "\n",
    "for item in dsTrainlower:\n",
    "   itemindex = dsTrainlower.index(item)\n",
    "   for word in item:\n",
    "       wordindex = vocabTrain.index(word)\n",
    "       Matrix[itemindex, wordindex] = Matrix[itemindex, wordindex] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 2.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Naive Bayes trainen & testen\n",
    "\n",
    "  * Maak een Multinomial Naive Bayes classifier uit SK-Learn aan.\n",
    "  * Train de classifier op de count vectorized train data en bijbehorende targets uit de training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(Matrix, datasetTrain.target)\n",
    "#Matrix is de feature vector x in de Naive Bayes\n",
    "#datasetTrain.Target is de klasse c in de Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Classificeer de test data met behulp van de getrainde Naive Bayes.\n",
    "  * Bereken de gemiddelde F1-score (average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WordFreqTe=np.zeros((len(datasetTest.data),len(vocabTrain)))\n",
    "WordFreqTe.shape\n",
    "for idx1, tewords in enumerate(dsTestlower):\n",
    "    for idx2, vword in enumerate(vocabTrain):\n",
    "        WordFreqTe[idx1,idx2]=tewords.count(vword)\n",
    "#print(WordFreqTe)\n",
    "\n",
    "ypred =clf.predict(WordFreqTe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors = 1 - f1_score(datasetTest.target,ypred , average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816516111008\n"
     ]
    }
   ],
   "source": [
    "print(1-errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[293,  99],\n",
       "       [ 43, 342]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(datasetTest.target,ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF-IDF vectorizer\n",
    "\n",
    "In bovenstaande opdracht bestond een feature vector uit het aantal keer dat elk woord voorkomt in een artikel. Alhoewel de methode aardig werkt, wordt hiermee niet meegenomen dat sommige woorden belangrijker zijn om documenten van elkaar te onderscheiden. De term frequency–inverse document frequency neemt dit wel mee.\n",
    "\n",
    "  * Maak een TF-IDF vectorizer uit SK-Learn aan.\n",
    "  * Train de vectorizer op de oorspronkelijke nieuwsgroep artikelen (dus niet de gesplitste lijst uit opdracht 2).\n",
    "  * Transformeer de train en de test data naar TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(datasetTrain.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(datasetTest.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Maak een Multinomial Naive Bayes classifier uit SK-Learn aan.\n",
    "  * Train de classifier op de TF-IDF vectorized train data en bijbehorende targets uit de training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfNB2 = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfNB2.fit(X_train, datasetTrain.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "  * Classificeer de test data met behulp van de getrainde Naive Bayes.\n",
    "  * Bereken de gemiddelde F1-score (average='macro'). Vergelijk dit met de score van de count vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ypred2 =clfNB2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors = 1 - f1_score(datasetTest.target,ypred2 , average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822153722778\n"
     ]
    }
   ],
   "source": [
    "print(1-errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Kun je de score nog verder verbeteren door stopwoorden te verwijderen, of door de minimale of maximale document frequency aan te passen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822153722778\n"
     ]
    }
   ],
   "source": [
    "vectorizerSW = TfidfVectorizer(stop_words='english')\n",
    "X_trainSW = vectorizerSW.fit_transform(datasetTrain.data)\n",
    "X_testSW = vectorizerSW.transform(datasetTest.data)\n",
    "clfNB3 = MultinomialNB()\n",
    "clfNB3.fit(X_train, datasetTrain.target)\n",
    "ypred3 =clfNB3.predict(X_test)\n",
    "errors3 = 1 - f1_score(datasetTest.target,ypred3 , average='macro')\n",
    "print(1-errors3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822153722778\n"
     ]
    }
   ],
   "source": [
    "vectorizerSWdfmin = TfidfVectorizer(max_df = 0.9,min_df = 0.02,stop_words='english')\n",
    "X_trainSWdfmin = vectorizerSWdfmin.fit_transform(datasetTrain.data)\n",
    "X_testSWdfmin = vectorizerSWdfmin.transform(datasetTest.data)\n",
    "clfNB4 = MultinomialNB()\n",
    "clfNB4.fit(X_train, datasetTrain.target)\n",
    "ypred4=clfNB4.predict(X_test)\n",
    "errors4 = 1 - f1_score(datasetTest.target,ypred4 , average='macro')\n",
    "print(1-errors4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
